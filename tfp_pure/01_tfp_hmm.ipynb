{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tfd = tfp.distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inputs for emissions and transitions\n",
    "time_steps = 100  # Example number of time steps\n",
    "num_states = 3    # Number of latent states\n",
    "dim_inputs_emission = 2\n",
    "\n",
    "# Input-driven emissions (external inputs u_emm_t for emission)\n",
    "u_emm = tf.random.uniform([time_steps, dim_inputs_emission])\n",
    "\n",
    "# Input-driven transitions (external inputs u_tr_t for transition)\n",
    "dim_inputs_transition = 2\n",
    "u_tr = tf.random.uniform([time_steps, dim_inputs_transition])\n",
    "\n",
    "# Define model parameters for emission and transition\n",
    "mu_emission = tf.Variable(tf.random.normal([num_states, dim_inputs_emission]), name='mu_emission')\n",
    "kappa_emission = tf.Variable(tf.ones([num_states]), name='kappa_emission')\n",
    "\n",
    "A_transition = tf.Variable(tf.random.normal([num_states, num_states]), name='A_transition')\n",
    "B_transition = tf.Variable(tf.random.normal([num_states, dim_inputs_transition]), name='B_transition')\n",
    "\n",
    "# Define initial state prior\n",
    "initial_state_logits = tf.Variable(tf.zeros([num_states]), name='initial_state_logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'event_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tfd\u001b[38;5;241m.\u001b[39mHiddenMarkovModel(\n\u001b[1;32m     19\u001b[0m         initial_distribution\u001b[38;5;241m=\u001b[39mtfd\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39minitial_state_logits),\n\u001b[1;32m     20\u001b[0m         transition_distribution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m state: transition_distribution_fn(state, u_tr[state]),\n\u001b[1;32m     21\u001b[0m         observation_distribution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m state: emission_distribution_fn(state, u_emm[state]),\n\u001b[1;32m     22\u001b[0m         num_steps\u001b[38;5;241m=\u001b[39mtime_steps\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create the HMM\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m hmm \u001b[38;5;241m=\u001b[39m \u001b[43mmake_hmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mmake_hmm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_hmm\u001b[39m():\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtfd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHiddenMarkovModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_distribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state_logits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransition_distribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition_distribution_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_distribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43memission_distribution_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_emm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_steps\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssm/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssm/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py:342\u001b[0m, in \u001b[0;36m_DistributionMeta.__new__.<locals>.wrapped_init\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to have things set in `self` before `__init__` is\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# called, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m \u001b[43mdefault_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to override things set in `self` by subclass\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# `__init__`, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m   \u001b[38;5;66;03m# We prefer subclasses will set `parameters = dict(locals())` because\u001b[39;00m\n\u001b[1;32m    347\u001b[0m   \u001b[38;5;66;03m# this has nearly zero overhead. However, failing to do this, we will\u001b[39;00m\n\u001b[1;32m    348\u001b[0m   \u001b[38;5;66;03m# resolve the input arguments dynamically and only when needed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssm/lib/python3.10/site-packages/tensorflow_probability/python/distributions/hidden_markov_model.py:209\u001b[0m, in \u001b[0;36mHiddenMarkovModel.__init__\u001b[0;34m(self, initial_distribution, transition_distribution, observation_distribution, num_steps, validate_args, allow_nan_stats, time_varying_transition_distribution, time_varying_observation_distribution, mask, name)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`num_steps` must be a scalar but it has rank \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    205\u001b[0m             np\u001b[38;5;241m.\u001b[39mndim(num_steps_)))\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_static_event_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensorShape(\n\u001b[1;32m    208\u001b[0m         [num_steps_])\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m--> 209\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_observation_distribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_shape\u001b[49m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_static_event_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensorShape(\n\u001b[1;32m    212\u001b[0m       [\u001b[38;5;28;01mNone\u001b[39;00m])\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m    213\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observation_distribution\u001b[38;5;241m.\u001b[39mevent_shape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'event_shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "def emission_distribution_fn(state, u_emm_t):\n",
    "    \"\"\"\n",
    "    Creates a von Mises emission distribution for the given state, using u_emm_t to modify the mean.\n",
    "    \"\"\"\n",
    "    mu = tf.reduce_sum(mu_emission[state] * u_emm_t)  # Custom emission component: mean driven by input\n",
    "    kappa = kappa_emission[state]\n",
    "    return tfd.VonMises(loc=mu, concentration=kappa)\n",
    "\n",
    "def transition_distribution_fn(state, u_tr_t):\n",
    "    \"\"\"\n",
    "    Creates a transition distribution given the current state and the input u_tr_t.\n",
    "    \"\"\"\n",
    "    logits = tf.reduce_sum(A_transition[state] * state + u_tr_t @ tf.transpose(B_transition))\n",
    "    return tfd.Categorical(logits=logits)\n",
    "\n",
    "# Build the HMM\n",
    "def make_hmm():\n",
    "    return tfd.HiddenMarkovModel(\n",
    "        initial_distribution=tfd.Categorical(logits=initial_state_logits),\n",
    "        transition_distribution=lambda state: transition_distribution_fn(state, u_tr[state]),\n",
    "        observation_distribution=lambda state: emission_distribution_fn(state, u_emm[state]),\n",
    "        num_steps=time_steps\n",
    "    )\n",
    "\n",
    "# Create the HMM\n",
    "hmm = make_hmm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target data (simulated observations for training)\n",
    "target_data = tf.random.uniform([time_steps], maxval=num_states, dtype=tf.int32)\n",
    "\n",
    "# Define the loss function\n",
    "def neg_log_likelihood():\n",
    "    return -tf.reduce_mean(hmm.log_prob(target_data))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = neg_log_likelihood()\n",
    "    gradients = tape.gradient(loss, [initial_state_logits, mu_emission, kappa_emission, A_transition, B_transition])\n",
    "    optimizer.apply_gradients(zip(gradients, [initial_state_logits, mu_emission, kappa_emission, A_transition, B_transition]))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Sample from the HMM or compute log probabilities\n",
    "samples = hmm.sample()\n",
    "log_prob = hmm.log_prob(samples)\n",
    "\n",
    "print(\"Samples:\", samples)\n",
    "print(\"Log Probability:\", log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
