{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glm_hmm_utils.py\n",
    "\n",
    "# Functions to assist with GLM-HMM model fitting\n",
    "import sys\n",
    "import ssm\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "\n",
    "def load_data(animal_file):\n",
    "    container = np.load(animal_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    inpt = data[0]\n",
    "    y = data[1]\n",
    "    session = data[2]\n",
    "    return inpt, y, session\n",
    "\n",
    "\n",
    "def load_cluster_arr(cluster_arr_file):\n",
    "    container = np.load(cluster_arr_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    cluster_arr = data[0]\n",
    "    return cluster_arr\n",
    "\n",
    "\n",
    "def load_glm_vectors(glm_vectors_file):\n",
    "    container = np.load(glm_vectors_file)\n",
    "    data = [container[key] for key in container]\n",
    "    loglikelihood_train = data[0]\n",
    "    recovered_weights = data[1]\n",
    "    return loglikelihood_train, recovered_weights\n",
    "\n",
    "\n",
    "def load_global_params(global_params_file):\n",
    "    container = np.load(global_params_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    global_params = data[0]\n",
    "    return global_params\n",
    "\n",
    "\n",
    "def partition_data_by_session(inpt, y, mask, session):\n",
    "    '''\n",
    "    Partition inpt, y, mask by session\n",
    "    :param inpt: arr of size TxM\n",
    "    :param y:  arr of size T x D\n",
    "    :param mask: Boolean arr of size T indicating if element is violation or\n",
    "    not\n",
    "    :param session: list of size T containing session ids\n",
    "    :return: list of inpt arrays, data arrays and mask arrays, where the\n",
    "    number of elements in list = number of sessions and each array size is\n",
    "    number of trials in session\n",
    "    '''\n",
    "    inputs = []\n",
    "    datas = []\n",
    "    indexes = np.unique(session, return_index=True)[1]\n",
    "    unique_sessions = [session[index] for index in sorted(indexes)]\n",
    "    counter = 0\n",
    "    masks = []\n",
    "    for sess in unique_sessions:\n",
    "        idx = np.where(session == sess)[0]\n",
    "        counter += len(idx)\n",
    "        inputs.append(inpt[idx, :])\n",
    "        datas.append(y[idx, :])\n",
    "        masks.append(mask[idx, :])\n",
    "    assert counter == inpt.shape[0], \"not all trials assigned to session!\"\n",
    "    return inputs, datas, masks\n",
    "\n",
    "\n",
    "def load_session_fold_lookup(file_path):\n",
    "    container = np.load(file_path, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    session_fold_lookup_table = data[0]\n",
    "    return session_fold_lookup_table\n",
    "\n",
    "\n",
    "def load_animal_list(file):\n",
    "    container = np.load(file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    animal_list = data[0]\n",
    "    return animal_list\n",
    "\n",
    "\n",
    "def create_violation_mask(violation_idx, T):\n",
    "    \"\"\"\n",
    "    Return indices of nonviolations and also a Boolean mask for inclusion (1\n",
    "    = nonviolation; 0 = violation)\n",
    "    :param test_idx:\n",
    "    :param T:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mask = np.array([i not in violation_idx for i in range(T)])\n",
    "    nonviolation_idx = np.arange(T)[mask]\n",
    "    mask = mask + 0\n",
    "    assert len(nonviolation_idx) + len(\n",
    "        violation_idx\n",
    "    ) == T, \"violation and non-violation idx do not include all dta!\"\n",
    "    return nonviolation_idx, np.expand_dims(mask, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_glm_hmm_job(inpt, y, session, mask, session_fold_lookup_table, K, D,\n",
    "                       C, N_em_iters, transition_alpha, prior_sigma, fold,\n",
    "                       iter, global_fit, init_param_file, save_directory):\n",
    "    print(\"Starting inference with K = \" + str(K) + \"; Fold = \" + str(fold) +\n",
    "          \"; Iter = \" + str(iter))\n",
    "    sys.stdout.flush()\n",
    "    sessions_to_keep = session_fold_lookup_table[np.where(\n",
    "        session_fold_lookup_table[:, 1] != fold), 0]\n",
    "    idx_this_fold = [str(sess) in sessions_to_keep for sess in session]\n",
    "    this_inpt, this_y, this_session, this_mask = inpt[idx_this_fold, :], \\\n",
    "                                                 y[idx_this_fold, :], \\\n",
    "                                                 session[idx_this_fold], \\\n",
    "                                                 mask[idx_this_fold]\n",
    "    # Only do this so that errors are avoided - these y values will not\n",
    "    # actually be used for anything (due to violation mask)\n",
    "    this_y[np.where(this_y == -1), :] = 1\n",
    "    inputs, datas, masks = partition_data_by_session(\n",
    "        this_inpt, this_y, this_mask, this_session)\n",
    "    # Read in GLM fit if global_fit = True:\n",
    "    _, params_for_initialization = load_glm_vectors(init_param_file)\n",
    "    M = this_inpt.shape[1]\n",
    "    npr.seed(iter)\n",
    "    return fit_glm_hmm(datas,\n",
    "                inputs,\n",
    "                masks,\n",
    "                K,\n",
    "                D,\n",
    "                M,\n",
    "                C,\n",
    "                N_em_iters,\n",
    "                transition_alpha,\n",
    "                prior_sigma,\n",
    "                global_fit,\n",
    "                params_for_initialization,\n",
    "                save_title=save_directory + 'glm_hmm_raw_parameters_itr_' +\n",
    "                           str(iter) + '.npz')\n",
    "    \n",
    "def fit_glm_hmm(datas, inputs, masks, K, D, M, C, N_em_iters,\n",
    "                transition_alpha, prior_sigma, global_fit,\n",
    "                params_for_initialization, save_title):\n",
    "    '''\n",
    "    Instantiate and fit GLM-HMM model\n",
    "    :param datas:\n",
    "    :param inputs:\n",
    "    :param masks:\n",
    "    :param K:\n",
    "    :param D:\n",
    "    :param M:\n",
    "    :param C:\n",
    "    :param N_em_iters:\n",
    "    :param global_fit:\n",
    "    :param glm_vectors:\n",
    "    :param save_title:\n",
    "    :return:\n",
    "    '''\n",
    "    # Prior variables\n",
    "    # Choice of prior\n",
    "    this_hmm = ssm.HMM(K,\n",
    "                        D,\n",
    "                        M,\n",
    "                        observations=\"input_driven_obs\",\n",
    "                        observation_kwargs=dict(C=C,\n",
    "                                                prior_sigma=prior_sigma),\n",
    "                        transitions=\"sticky\",\n",
    "                        transition_kwargs=dict(alpha=transition_alpha,\n",
    "                                                kappa=0))\n",
    "    # Initialize observation weights as GLM weights with some noise:\n",
    "    glm_vectors_repeated = np.tile(params_for_initialization, (K, 1, 1))\n",
    "    glm_vectors_with_noise = glm_vectors_repeated + np.random.normal(\n",
    "        0, 0.2, glm_vectors_repeated.shape)\n",
    "    this_hmm.observations.params = glm_vectors_with_noise\n",
    "    print(\"=== fitting GLM-HMM ========\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print(\"datas shape: \" + str(np.array(datas).shape))\n",
    "    print(\"inputs shape: \" + str(np.array(inputs).shape))\n",
    "    print(\"masks shape: \" + str(np.array(masks).shape))\n",
    "\n",
    "    return datas, inputs, masks\n",
    "    \n",
    "\n",
    "    # Fit this HMM and calculate marginal likelihood\n",
    "    # lls = this_hmm.fit(datas,\n",
    "    #                    inputs=inputs,\n",
    "    #                    masks=masks,\n",
    "    #                    method=\"em\",\n",
    "    #                    num_iters=N_em_iters,\n",
    "    #                    initialize=False,\n",
    "    #                    tolerance=10 ** -4)\n",
    "    # # Save raw parameters of HMM, as well as loglikelihood during training\n",
    "    # np.savez(save_title, this_hmm.params, lls)\n",
    "    # return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference with K = 2; Fold = 0; Iter = 0\n",
      "=== fitting GLM-HMM ========\n",
      "datas shape: (1618, 90, 1)\n",
      "inputs shape: (1618, 90, 4)\n",
      "masks shape: (1618, 90, 1)\n"
     ]
    }
   ],
   "source": [
    "# 1_run_inference_global_fit_ibl.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import autograd.numpy as np\n",
    "\n",
    "\n",
    "D = 1  # data (observations) dimension\n",
    "C = 2  # number of output types/categories\n",
    "N_em_iters = 300  # number of EM iterations\n",
    "\n",
    "USE_CLUSTER = False\n",
    "\n",
    "data_dir = '/home/rudra/Desktop/markov_models/data/ibl/data_for_cluster/'\n",
    "results_dir = '/home/rudra/Desktop/markov_models/results/ibl_global_fit/'\n",
    "\n",
    "if USE_CLUSTER:\n",
    "    z = int(sys.argv[1])\n",
    "else:\n",
    "    z = 0\n",
    "\n",
    "num_folds = 5\n",
    "global_fit = True\n",
    "# perform mle => set transition_alpha to 1\n",
    "transition_alpha = 1\n",
    "prior_sigma = 100\n",
    "\n",
    "# Load external files:\n",
    "cluster_arr_file = data_dir + 'cluster_job_arr.npz'\n",
    "# Load cluster array job parameters:\n",
    "cluster_arr = load_cluster_arr(cluster_arr_file)\n",
    "[K, fold, iter] = cluster_arr[z]\n",
    "\n",
    "#  read in data and train/test split\n",
    "animal_file = data_dir + 'all_animals_concat.npz'\n",
    "session_fold_lookup_table = load_session_fold_lookup(\n",
    "    data_dir + 'all_animals_concat_session_fold_lookup.npz')\n",
    "\n",
    "inpt, y, session = load_data(animal_file)\n",
    "#  append a column of ones to inpt to represent the bias covariate:\n",
    "inpt = np.hstack((inpt, np.ones((len(inpt),1))))\n",
    "y = y.astype('int')\n",
    "# Identify violations for exclusion:\n",
    "violation_idx = np.where(y == -1)[0]\n",
    "nonviolation_idx, mask = create_violation_mask(violation_idx,\n",
    "                                                inpt.shape[0])\n",
    "\n",
    "#  GLM weights to use to initialize GLM-HMM\n",
    "init_param_file = results_dir + '/GLM/fold_' + str(\n",
    "    fold) + '/variables_of_interest_iter_0.npz'\n",
    "\n",
    "# create save directory for this initialization/fold combination:\n",
    "save_directory = results_dir + '/GLM_HMM_K_' + str(\n",
    "    K) + '/' + 'fold_' + str(fold) + '/' + '/iter_' + str(iter) + '/'\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "data, inputs, masks = launch_glm_hmm_job(inpt,\n",
    "                                            y,\n",
    "                                            session,\n",
    "                                            mask,\n",
    "                                            session_fold_lookup_table,\n",
    "                                            K,\n",
    "                                            D,\n",
    "                                            C,\n",
    "                                            N_em_iters,\n",
    "                                            transition_alpha,\n",
    "                                            prior_sigma,\n",
    "                                            fold,\n",
    "                                            iter,\n",
    "                                            global_fit,\n",
    "                                            init_param_file,\n",
    "                                            save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1618, 90, 1), (1618, 90, 4), (1618, 90, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "inputs = np.array(inputs)\n",
    "masks = np.array(masks)\n",
    "\n",
    "data.shape, inputs.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[7].reshape(-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The inputs values\n",
    "\n",
    "```\n",
    "def create_design_mat(choice, stim_left, stim_right, rewarded):\n",
    "    # Create unnormalized_inpt: with first column = stim_right - stim_left,\n",
    "    # second column as past choice, third column as WSLS\n",
    "    \n",
    "```\n",
    "\n",
    "# stim_right - stim_left, past choice, WSLS, Bias\n",
    "\n",
    "\n",
    "```\n",
    "wsls: vector of size T, entries are in {-1, 1}.  1 corresponds to\n",
    "previous choice = right and success OR previous choice = left and\n",
    "failure; -1 corresponds to previous choice = left and success OR previous choice = right and failure\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.60674147e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.60674147e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        5.24688844e-01, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        1.32007349e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        5.24688844e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.60674147e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -2.60674147e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -5.22461810e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -5.22461810e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.60674147e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -5.22461810e-01, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -5.22461810e-01,  1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01,  1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.60674147e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.32007349e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00,  1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -5.22461810e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -5.22461810e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        5.24688844e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        5.24688844e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -5.22461810e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -5.22461810e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        5.24688844e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        5.24688844e-01, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        5.24688844e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -2.60674147e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.32007349e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -2.60674147e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        1.32007349e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        5.24688844e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        1.32007349e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.09318779e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        1.11351692e-03, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -1.29780315e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "        5.24688844e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "       -2.60674147e-01, -1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        2.09541483e+00, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00,\n",
       "       -2.60674147e-01,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "        2.62901181e-01, -1.00000000e+00, -1.00000000e+00,  1.00000000e+00])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glmhmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
